{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Use FLAML to Tune ChatGPT\n",
    "\n",
    "In this notebook, we tune OpenAI ChatGPT model for math problem solving. We use [the MATH benchmark](https://crfm.stanford.edu/helm/latest/?group=math_chain_of_thought) for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the [openai] option:\n",
    "```bash\n",
    "pip install flaml[openai]==1.2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
     "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
     "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
     "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install flaml[openai]==1.2.0 datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.324240Z",
     "iopub.status.busy": "2023-02-13T23:40:52.323783Z",
     "iopub.status.idle": "2023-02-13T23:40:52.330570Z",
     "shell.execute_reply": "2023-02-13T23:40:52.329750Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI API key here>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following to use Azure OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.333547Z",
     "iopub.status.busy": "2023-02-13T23:40:52.333249Z",
     "iopub.status.idle": "2023-02-13T23:40:52.336508Z",
     "shell.execute_reply": "2023-02-13T23:40:52.335858Z"
    }
   },
   "outputs": [],
   "source": [
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = \"https://<your_endpoint>.openai.azure.com/\"\n",
    "# openai.api_version = \"2022-12-01\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "First, we load the competition_math dataset. The dataset contains 457 \"Level 1\" examples. We use a random sample of 20 examples for tuning the generation hyperparameters and the remaining for evaluation. We use one demonstration example in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.339977Z",
     "iopub.status.busy": "2023-02-13T23:40:52.339556Z",
     "iopub.status.idle": "2023-02-13T23:40:54.603349Z",
     "shell.execute_reply": "2023-02-13T23:40:54.602630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset competition_math (/home/vscode/.cache/huggingface/datasets/competition_math/default/1.0.0/2a2a2995c2847186883ecd64f69be7d602b8a6f6b51950624d4dc2263f93333b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d2c0e53b10466fb3c476f46ce2309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/vscode/.cache/huggingface/datasets/competition_math/default/1.0.0/2a2a2995c2847186883ecd64f69be7d602b8a6f6b51950624d4dc2263f93333b/cache-f1cfe8228271b121.arrow\n",
      "Loading cached shuffled indices for dataset at /home/vscode/.cache/huggingface/datasets/competition_math/default/1.0.0/2a2a2995c2847186883ecd64f69be7d602b8a6f6b51950624d4dc2263f93333b/cache-d155a2d38c23bd53.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tokens in tuning data's canonical solutions 128\n",
      "20 437\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "seed = 41\n",
    "data = datasets.load_dataset(\"competition_math\")\n",
    "train_data = data[\"train\"].shuffle(seed=seed)\n",
    "test_data = data[\"test\"].shuffle(seed=seed)\n",
    "n_tune_data = 20\n",
    "tune_data = [\n",
    "    {\n",
    "        \"problem\": train_data[x][\"problem\"],\n",
    "        \"solution\": train_data[x][\"solution\"],\n",
    "    }\n",
    "    for x in range(len(train_data)) if train_data[x][\"level\"] == \"Level 1\"\n",
    "][:n_tune_data]\n",
    "test_data = [\n",
    "    {\n",
    "        \"problem\": test_data[x][\"problem\"],\n",
    "        \"solution\": test_data[x][\"solution\"],\n",
    "    }\n",
    "    for x in range(len(test_data)) if test_data[x][\"level\"] == \"Level 1\"\n",
    "]\n",
    "input_field = \"problem\"\n",
    "output_fields = [\"solution\"]\n",
    "print(\"max tokens in tuning data's canonical solutions\", max([len(x[\"solution\"].split()) for x in tune_data]))\n",
    "print(len(tune_data), len(test_data))\n",
    "# prompt template\n",
    "prompts = [\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check a tuning example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.607152Z",
     "iopub.status.busy": "2023-02-13T23:40:54.606441Z",
     "iopub.status.idle": "2023-02-13T23:40:54.610504Z",
     "shell.execute_reply": "2023-02-13T23:40:54.609759Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find $\\log_{10} 40 +\\log_{10} 25$.\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"problem\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one example of the canonical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.613590Z",
     "iopub.status.busy": "2023-02-13T23:40:54.613168Z",
     "iopub.status.idle": "2023-02-13T23:40:54.616873Z",
     "shell.execute_reply": "2023-02-13T23:40:54.616193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using $\\log x+\\log y=\\log xy,$ we get that $\\log_{10} 40+\\log_{10} 25=\\log_{10}(40\\cdot 25)=\\log 1000.$ That means we want $x$ where $10^x=1000,$ which means $x=3.$ Therefore, $\\log_{10} 40+\\log_{10} 25=\\boxed{3}.$\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"solution\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Success Metric\n",
    "\n",
    "Before we start tuning, we need to define the success metric we want to opotimize. For each math task, we use voting to select a response with the most common answers out of all the generated responses. If it has an equivalent answer to the canonical solution, we consider the task as successfully solved. Then we can optimize the mean success rate of a collection of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.626998Z",
     "iopub.status.busy": "2023-02-13T23:40:54.626593Z",
     "iopub.status.idle": "2023-02-13T23:40:54.631383Z",
     "shell.execute_reply": "2023-02-13T23:40:54.630770Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def remove_boxed(string: str) -> Optional[str]:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Extract the text within a \\\\boxed{...} environment.\n",
    "    Example:\n",
    "    >>> remove_boxed(\\\\boxed{\\\\frac{2}{3}})\n",
    "    \\\\frac{2}{3}\n",
    "    \"\"\"\n",
    "    left = \"\\\\boxed{\"\n",
    "    try:\n",
    "        assert string[: len(left)] == left\n",
    "        assert string[-1] == \"}\"\n",
    "        return string[len(left) : -1]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def last_boxed_only_string(string: str) -> Optional[str]:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Extract the last \\\\boxed{...} or \\\\fbox{...} element from a string.\n",
    "    \"\"\"\n",
    "    idx = string.rfind(\"\\\\boxed\")\n",
    "    if idx < 0:\n",
    "        idx = string.rfind(\"\\\\fbox\")\n",
    "        if idx < 0:\n",
    "            return None\n",
    "\n",
    "    i = idx\n",
    "    right_brace_idx = None\n",
    "    num_left_braces_open = 0\n",
    "    while i < len(string):\n",
    "        if string[i] == \"{\":\n",
    "            num_left_braces_open += 1\n",
    "        if string[i] == \"}\":\n",
    "            num_left_braces_open -= 1\n",
    "            if num_left_braces_open == 0:\n",
    "                right_brace_idx = i\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    if right_brace_idx is None:\n",
    "        retval = None\n",
    "    else:\n",
    "        retval = string[idx : right_brace_idx + 1]\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "def _fix_fracs(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Reformat fractions.\n",
    "    Examples:\n",
    "    >>> _fix_fracs(\"\\\\frac1b\")\n",
    "    \\frac{1}{b}\n",
    "    >>> _fix_fracs(\"\\\\frac12\")\n",
    "    \\frac{1}{2}\n",
    "    >>> _fix_fracs(\"\\\\frac1{72}\")\n",
    "    \\frac{1}{72}\n",
    "    \"\"\"\n",
    "    substrs = string.split(\"\\\\frac\")\n",
    "    new_str = substrs[0]\n",
    "    if len(substrs) > 1:\n",
    "        substrs = substrs[1:]\n",
    "        for substr in substrs:\n",
    "            new_str += \"\\\\frac\"\n",
    "            if substr[0] == \"{\":\n",
    "                new_str += substr\n",
    "            else:\n",
    "                try:\n",
    "                    assert len(substr) >= 2\n",
    "                except Exception:\n",
    "                    return string\n",
    "                a = substr[0]\n",
    "                b = substr[1]\n",
    "                if b != \"{\":\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n",
    "                else:\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}\" + b + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}\" + b\n",
    "    string = new_str\n",
    "    return string\n",
    "\n",
    "\n",
    "def _fix_a_slash_b(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Reformat fractions formatted as a/b to \\\\frac{a}{b}.\n",
    "    Example:\n",
    "    >>> _fix_a_slash_b(\"2/3\")\n",
    "    \\frac{2}{3}\n",
    "    \"\"\"\n",
    "    if len(string.split(\"/\")) != 2:\n",
    "        return string\n",
    "    a_str = string.split(\"/\")[0]\n",
    "    b_str = string.split(\"/\")[1]\n",
    "    try:\n",
    "        a = int(a_str)\n",
    "        b = int(b_str)\n",
    "        assert string == \"{}/{}\".format(a, b)\n",
    "        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n",
    "        return new_string\n",
    "    except Exception:\n",
    "        return string\n",
    "\n",
    "\n",
    "def _remove_right_units(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Remove units (on the right).\n",
    "    \"\\\\text{ \" only ever occurs (at least in the val set) when describing units.\n",
    "    \"\"\"\n",
    "    if \"\\\\text{ \" in string:\n",
    "        splits = string.split(\"\\\\text{ \")\n",
    "        assert len(splits) == 2\n",
    "        return splits[0]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "def _fix_sqrt(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Reformat square roots.\n",
    "    Example:\n",
    "    >>> _fix_sqrt(\"\\\\sqrt3\")\n",
    "    \\sqrt{3}\n",
    "    \"\"\"\n",
    "    if \"\\\\sqrt\" not in string:\n",
    "        return string\n",
    "    splits = string.split(\"\\\\sqrt\")\n",
    "    new_string = splits[0]\n",
    "    for split in splits[1:]:\n",
    "        if split[0] != \"{\":\n",
    "            a = split[0]\n",
    "            new_substr = \"\\\\sqrt{\" + a + \"}\" + split[1:]\n",
    "        else:\n",
    "            new_substr = \"\\\\sqrt\" + split\n",
    "        new_string += new_substr\n",
    "    return new_string\n",
    "\n",
    "\n",
    "def _strip_string(string: str) -> str:\n",
    "    \"\"\"Source: https://github.com/hendrycks/math\n",
    "    Apply the reformatting helper functions above.\n",
    "    \"\"\"\n",
    "    # linebreaks\n",
    "    string = string.replace(\"\\n\", \"\")\n",
    "    # print(string)\n",
    "\n",
    "    # remove inverse spaces\n",
    "    string = string.replace(\"\\\\!\", \"\")\n",
    "    # print(string)\n",
    "\n",
    "    # replace \\\\ with \\\n",
    "    string = string.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    # print(string)\n",
    "\n",
    "    # replace tfrac and dfrac with frac\n",
    "    string = string.replace(\"tfrac\", \"frac\")\n",
    "    string = string.replace(\"dfrac\", \"frac\")\n",
    "    # print(string)\n",
    "\n",
    "    # remove \\left and \\right\n",
    "    string = string.replace(\"\\\\left\", \"\")\n",
    "    string = string.replace(\"\\\\right\", \"\")\n",
    "    # print(string)\n",
    "\n",
    "    # Remove circ (degrees)\n",
    "    string = string.replace(\"^{\\\\circ}\", \"\")\n",
    "    string = string.replace(\"^\\\\circ\", \"\")\n",
    "\n",
    "    # remove dollar signs\n",
    "    string = string.replace(\"\\\\$\", \"\")\n",
    "\n",
    "    # remove units (on the right)\n",
    "    string = _remove_right_units(string)\n",
    "\n",
    "    # remove percentage\n",
    "    string = string.replace(\"\\\\%\", \"\")\n",
    "    string = string.replace(\"\\%\", \"\")\n",
    "\n",
    "    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n",
    "    string = string.replace(\" .\", \" 0.\")\n",
    "    string = string.replace(\"{.\", \"{0.\")\n",
    "    # if empty, return empty string\n",
    "    if len(string) == 0:\n",
    "        return string\n",
    "    if string[0] == \".\":\n",
    "        string = \"0\" + string\n",
    "\n",
    "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
    "    if len(string.split(\"=\")) == 2:\n",
    "        if len(string.split(\"=\")[0]) <= 2:\n",
    "            string = string.split(\"=\")[1]\n",
    "\n",
    "    # fix sqrt3 --> sqrt{3}\n",
    "    string = _fix_sqrt(string)\n",
    "\n",
    "    # remove spaces\n",
    "    string = string.replace(\" \", \"\")\n",
    "\n",
    "    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc.\n",
    "    # Even works with \\frac1{72} (but not \\frac{72}1).\n",
    "    # Also does a/b --> \\\\frac{a}{b}\n",
    "    string = _fix_fracs(string)\n",
    "\n",
    "    # manually change 0.5 --> \\frac{1}{2}\n",
    "    if string == \"0.5\":\n",
    "        string = \"\\\\frac{1}{2}\"\n",
    "\n",
    "    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y\n",
    "    string = _fix_a_slash_b(string)\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_answer(solution: Optional[str]) -> Optional[str]:\n",
    "    if solution is None:\n",
    "        return None\n",
    "    last_boxed = last_boxed_only_string(solution)\n",
    "    if last_boxed is None:\n",
    "        return None\n",
    "    answer = remove_boxed(last_boxed)\n",
    "    if answer is None:\n",
    "        return None\n",
    "    return answer\n",
    "\n",
    "\n",
    "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float:\n",
    "    \"\"\"Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in\n",
    "    - units\n",
    "    - fractions\n",
    "    - square roots\n",
    "    - superfluous LaTeX.\n",
    "    Source: https://github.com/hendrycks/math\n",
    "    \"\"\"\n",
    "    if str1 is None and str2 is None:\n",
    "        print(\"WARNING: Both None\")\n",
    "        return 1.0\n",
    "    if str1 is None or str2 is None:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        ss1 = _strip_string(str1)\n",
    "        ss2 = _strip_string(str2)\n",
    "        return float(ss1 == ss2)\n",
    "    except Exception:\n",
    "        return float(str1 == str2)\n",
    "\n",
    "\n",
    "def is_equiv_chain_of_thought(str1: str, str2: str) -> float:\n",
    "    \"\"\"Strips the solution first before calling `is_equiv`.\"\"\"\n",
    "    ans1 = get_answer(str1)\n",
    "    ans2 = get_answer(str2)\n",
    "\n",
    "    return is_equiv(ans1, ans2)\n",
    "\n",
    "\n",
    "def success_metrics(responses, solution, **args):\n",
    "    \"\"\"Check if each response is correct.\n",
    "    \n",
    "    Args:\n",
    "        responses (list): The list of responses.\n",
    "        solution (str): The canonical solution.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The success metrics.\n",
    "    \"\"\"\n",
    "    success_list = []\n",
    "    n = len(responses)\n",
    "    for i in range(n):\n",
    "        response = responses[i]\n",
    "        succeed = is_equiv_chain_of_thought(response, solution)\n",
    "        success_list.append(succeed)\n",
    "    # voting\n",
    "    answers = {}\n",
    "    for i in range(n):\n",
    "        equiv = i\n",
    "        if get_answer(responses[i]) is None:\n",
    "            # ignore None answers\n",
    "            continue\n",
    "        for j in answers:\n",
    "            if is_equiv_chain_of_thought(responses[i], responses[j]):\n",
    "                equiv = j\n",
    "                break\n",
    "        if equiv in answers:\n",
    "            answers[equiv] += 1\n",
    "        else:\n",
    "            answers[equiv] = 1\n",
    "    # find the answer with highest votes in answers\n",
    "    answer = max(answers.items(), key=lambda x: x[1], default=(0, 0))[0]\n",
    "    # check if the answer is correct\n",
    "    success_vote = is_equiv_chain_of_thought(responses[answer], solution)\n",
    "    return {\n",
    "        \"expected_success\": 1 - pow(1 - sum(success_list) / n, n),\n",
    "        \"success\": any(s for s in success_list),\n",
    "        \"success_vote\": success_vote,\n",
    "        \"voted_answer\": responses[answer],\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use the tuning data to find a good configuration\n",
    "\n",
    "### Import the oai and tune subpackages from flaml.\n",
    "\n",
    "FLAML has provided an API for hyperparameter optimization of OpenAI ChatGPT models: `oai.ChatCompletion.tune` and to make a request with the tuned config: `oai.ChatCompletion.create`. First, we import oai from flaml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.634335Z",
     "iopub.status.busy": "2023-02-13T23:40:54.633929Z",
     "iopub.status.idle": "2023-02-13T23:40:56.105700Z",
     "shell.execute_reply": "2023-02-13T23:40:56.105085Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from flaml import oai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For (local) reproducibility and cost efficiency, we cache responses from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:56.109177Z",
     "iopub.status.busy": "2023-02-13T23:40:56.108624Z",
     "iopub.status.idle": "2023-02-13T23:40:56.112651Z",
     "shell.execute_reply": "2023-02-13T23:40:56.112076Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "oai.ChatCompletion.set_cache(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a disk cache in \".cache/{seed}\". You can change `cache_path` in `set_cache()`. The cache for different seeds are stored separately.\n",
    "\n",
    "### Perform tuning\n",
    "\n",
    "The tuning will take a while to finish, depending on the optimization budget. The tuning will be performed under the specified optimization budgets.\n",
    "\n",
    "* `inference_budget` is the target average inference budget per instance in the benchmark. For example, 0.004 means the target inference budget is 0.004 dollars, which translates to 2000 tokens (input + output combined) if the gpt-3.5-turbo model is used.\n",
    "* `optimization_budget` is the total budget allowed to perform the tuning. For example, 1 means 1 dollars are allowed in total, which translates to 500K tokens for the gpt-3.5-turbo model.\n",
    "* `num_sumples` is the number of different hyperparameter configurations which is allowed to try. The tuning will stop after either num_samples trials or after optimization_budget dollars spent, whichever happens first. -1 means no hard restriction in the number of trials and the actual number is decided by `optimization_budget`.\n",
    "\n",
    "Users can specify tuning data, optimization metric, optimization mode, evaluation function, search spaces etc.. The default search space is:\n",
    "\n",
    "```python\n",
    "price1K = {\n",
    "    \"gpt-3.5-turbo\": 0.002,\n",
    "}\n",
    "\n",
    "default_search_space = {\n",
    "    \"model\": tune.choice(list(price1K.keys())),\n",
    "    \"temperature_or_top_p\": tune.choice(\n",
    "        [\n",
    "            {\"temperature\": tune.uniform(0, 1)},\n",
    "            {\"top_p\": tune.uniform(0, 1)},\n",
    "        ]\n",
    "    ),\n",
    "    \"max_tokens\": tune.lograndint(50, 1000),\n",
    "    \"n\": tune.randint(1, 100),\n",
    "    \"prompt\": \"{prompt}\",\n",
    "}\n",
    "```\n",
    "\n",
    "The default search space can be overridden by users' input.\n",
    "For example, the following code specifies a fixed prompt template and a list of stop sequences. For hyperparameters which don't appear in users' input, the default search space will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:56.115383Z",
     "iopub.status.busy": "2023-02-13T23:40:56.114975Z",
     "iopub.status.idle": "2023-02-13T23:41:55.045654Z",
     "shell.execute_reply": "2023-02-13T23:41:55.044973Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-22 14:05:31,904]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-22 14:05:31] {811} INFO - trial 1 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.36280922847807595}, 'max_tokens': 347, 'n': 10, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:07:23] {215} INFO - result: {'expected_success': 0.798285292865, 'success': 0.8, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.06480200000000001, 'cost': 0.06480200000000001, 'inference_cost': 0.0031621, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.36280922847807595}, 'max_tokens': 347, 'n': 10, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.36280922847807595}, 'config/max_tokens': 347, 'config/n': 10, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 111.40725612640381}\n",
      "[flaml.tune.tune: 03-22 14:07:23] {811} INFO - trial 2 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6336482349262754}, 'max_tokens': 470, 'n': 50, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:07:39] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.08049600000000001, 'cost': 0.015694, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6336482349262754}, 'max_tokens': 470, 'n': 50, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6336482349262754}, 'config/max_tokens': 470, 'config/n': 50, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 16.562241077423096}\n",
      "[flaml.tune.tune: 03-22 14:07:39] {811} INFO - trial 3 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7605307121989587}, 'max_tokens': 82, 'n': 9, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:08:38] {215} INFO - result: {'expected_success': 0.413787478983849, 'success': 0.5, 'success_vote': 0.45, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.11175400000000003, 'cost': 0.031258, 'inference_cost': 0.0015629, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.7605307121989587}, 'max_tokens': 82, 'n': 9, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.7605307121989587}, 'config/max_tokens': 82, 'config/n': 9, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 59.041048765182495}\n",
      "[flaml.tune.tune: 03-22 14:08:38] {811} INFO - trial 4 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.003948266327914451}, 'max_tokens': 231, 'n': 81, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.12621600000000005, 'cost': 0.014462000000000001, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.003948266327914451}, 'max_tokens': 231, 'n': 81, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.003948266327914451}, 'config/max_tokens': 231, 'config/n': 81, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 6.367751836776733}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {811} INFO - trial 5 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.29187606817063316}, 'max_tokens': 781, 'n': 71, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.29187606817063316}, 'max_tokens': 781, 'n': 71, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.29187606817063316}, 'config/max_tokens': 781, 'config/n': 71, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0005717277526855469}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {811} INFO - trial 6 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3733407600514692}, 'max_tokens': 375, 'n': 44, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3733407600514692}, 'max_tokens': 375, 'n': 44, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.3733407600514692}, 'config/max_tokens': 375, 'config/n': 44, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0005567073822021484}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {811} INFO - trial 7 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5131382425543909}, 'max_tokens': 350, 'n': 60, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.5131382425543909}, 'max_tokens': 350, 'n': 60, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.5131382425543909}, 'config/max_tokens': 350, 'config/n': 60, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0004477500915527344}\n",
      "[flaml.tune.tune: 03-22 14:08:45] {811} INFO - trial 8 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:10:01] {215} INFO - result: {'expected_success': 0.808418317029175, 'success': 0.85, 'success_vote': 0.85, 'voted_answer': '\\n\\nWe have that $\\\\|\\\\mathbf{v}\\\\| = 4,$ so\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.16798400000000008, 'cost': 0.041768, 'inference_cost': 0.0020884000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9086488808086682}, 'config/max_tokens': 129, 'config/n': 9, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 75.71505641937256}\n",
      "[flaml.tune.tune: 03-22 14:10:01] {811} INFO - trial 9 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8286813263076767}, 'max_tokens': 57, 'n': 63, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:10:04] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.17730400000000007, 'cost': 0.00932, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8286813263076767}, 'max_tokens': 57, 'n': 63, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8286813263076767}, 'config/max_tokens': 57, 'config/n': 63, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 3.8257486820220947}\n",
      "[flaml.tune.tune: 03-22 14:10:04] {811} INFO - trial 10 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.1989475396788123}, 'max_tokens': 650, 'n': 35, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:10:13] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.19205400000000009, 'cost': 0.01475, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.1989475396788123}, 'max_tokens': 650, 'n': 35, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.1989475396788123}, 'config/max_tokens': 650, 'config/n': 35, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 8.731875896453857}\n",
      "[flaml.tune.tune: 03-22 14:10:13] {811} INFO - trial 11 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8839364795611863}, 'max_tokens': 132, 'n': 17, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:11:37] {215} INFO - result: {'expected_success': 0.7791317580793126, 'success': 0.8, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe know that $\\\\|-3 \\\\mathbf{v}\\\\| = | -3 | \\\\cdot \\\\|\\\\mathbf{v}\\\\|.$ Since $\\\\|\\\\mathbf{v}\\\\| = 4,$ we have \\\\[\\\\|-3 \\\\mathbf{v}\\\\| = | -3 | \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.27019800000000005, 'cost': 0.07814400000000002, 'inference_cost': 0.0039072, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8839364795611863}, 'max_tokens': 132, 'n': 17, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8839364795611863}, 'config/max_tokens': 132, 'config/n': 17, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 84.14633345603943}\n",
      "[flaml.tune.tune: 03-22 14:11:37] {811} INFO - trial 12 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8211056578369285}, 'max_tokens': 78, 'n': 39, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:11:45] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.282774, 'cost': 0.012576, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.8211056578369285}, 'max_tokens': 78, 'n': 39, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.8211056578369285}, 'config/max_tokens': 78, 'config/n': 39, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 7.665576934814453}\n",
      "[flaml.tune.tune: 03-22 14:11:45] {811} INFO - trial 13 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0422875090290305}, 'max_tokens': 144, 'n': 3, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:13:05] {215} INFO - result: {'expected_success': 0.6018518518518519, 'success': 0.65, 'success_vote': 0.65, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.2998860000000001, 'cost': 0.017112, 'inference_cost': 0.0008556, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.0422875090290305}, 'max_tokens': 144, 'n': 3, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.0422875090290305}, 'config/max_tokens': 144, 'config/n': 3, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 79.9990644454956}\n",
      "[flaml.tune.tune: 03-22 14:13:05] {811} INFO - trial 14 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9952461897033992}, 'max_tokens': 140, 'n': 14, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:14:31] {215} INFO - result: {'expected_success': 0.8187068892044733, 'success': 0.85, 'success_vote': 0.8, 'voted_answer': '\\n\\nRecall that $\\\\|-3 \\\\mathbf{v}\\\\| = 3\\\\|\\\\mathbf{v}\\\\|.$ Thus, $\\\\|-3 \\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.$', 'total_cost': 0.36728000000000005, 'cost': 0.067394, 'inference_cost': 0.0033696999999999998, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9952461897033992}, 'max_tokens': 140, 'n': 14, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9952461897033992}, 'config/max_tokens': 140, 'config/n': 14, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 86.46698451042175}\n",
      "[flaml.tune.tune: 03-22 14:14:31] {811} INFO - trial 15 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9905338029815671}, 'max_tokens': 147, 'n': 25, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:14:53] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.38525000000000004, 'cost': 0.01797, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9905338029815671}, 'max_tokens': 147, 'n': 25, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9905338029815671}, 'config/max_tokens': 147, 'config/n': 25, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 21.18087410926819}\n",
      "[flaml.tune.tune: 03-22 14:14:53] {811} INFO - trial 16 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5533169427766509}, 'max_tokens': 98, 'n': 98, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:14:53] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.5533169427766509}, 'max_tokens': 98, 'n': 98, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.5533169427766509}, 'config/max_tokens': 98, 'config/n': 98, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0005028247833251953}\n",
      "[flaml.tune.tune: 03-22 14:14:53] {811} INFO - trial 17 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9892826883313328}, 'max_tokens': 221, 'n': 1, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:16:13] {215} INFO - result: {'expected_success': 0.55, 'success': 0.55, 'success_vote': 0.55, 'voted_answer': 'We have that $\\\\|-3 \\\\mathbf{v}\\\\| = |-3|\\\\|\\\\mathbf{v}\\\\| = 3\\\\|\\\\mathbf{v}\\\\| = 3\\\\cdot 4 = \\\\boxed{12}.$', 'total_cost': 0.394016, 'cost': 0.008766, 'inference_cost': 0.00043830000000000003, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9892826883313328}, 'max_tokens': 221, 'n': 1, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9892826883313328}, 'config/max_tokens': 221, 'config/n': 1, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 80.3955602645874}\n",
      "[flaml.tune.tune: 03-22 14:16:13] {811} INFO - trial 18 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6204584002665291}, 'max_tokens': 56, 'n': 23, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:17:04] {215} INFO - result: {'expected_success': 0.1460399069929787, 'success': 0.2, 'success_vote': 0.2, 'voted_answer': 'We have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.447748, 'cost': 0.053731999999999995, 'inference_cost': 0.0026866, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6204584002665291}, 'max_tokens': 56, 'n': 23, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6204584002665291}, 'config/max_tokens': 56, 'config/n': 23, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 51.32055115699768}\n",
      "[flaml.tune.tune: 03-22 14:17:04] {811} INFO - trial 19 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.32687425301877854}, 'max_tokens': 204, 'n': 13, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:18:59] {215} INFO - result: {'expected_success': 0.8962445287919472, 'success': 0.9, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.519372, 'cost': 0.07162400000000002, 'inference_cost': 0.0035812, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.32687425301877854}, 'max_tokens': 204, 'n': 13, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.32687425301877854}, 'config/max_tokens': 204, 'config/n': 13, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 114.26092863082886}\n",
      "[flaml.tune.tune: 03-22 14:18:59] {811} INFO - trial 20 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9915242842721368}, 'max_tokens': 331, 'n': 32, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:18:59] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9915242842721368}, 'max_tokens': 331, 'n': 32, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9915242842721368}, 'config/max_tokens': 331, 'config/n': 32, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.00041413307189941406}\n",
      "[flaml.tune.tune: 03-22 14:18:59] {811} INFO - trial 21 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6810064227782602}, 'max_tokens': 127, 'n': 5, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:20:21] {215} INFO - result: {'expected_success': 0.6362400000000001, 'success': 0.65, 'success_vote': 0.65, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\cdot \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.5435499999999999, 'cost': 0.024178, 'inference_cost': 0.0012089000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6810064227782602}, 'max_tokens': 127, 'n': 5, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6810064227782602}, 'config/max_tokens': 127, 'config/n': 5, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 82.1910355091095}\n",
      "[flaml.tune.tune: 03-22 14:20:21] {811} INFO - trial 22 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.04646269376572404}, 'max_tokens': 101, 'n': 50, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:20:21] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.04646269376572404}, 'max_tokens': 101, 'n': 50, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.04646269376572404}, 'config/max_tokens': 101, 'config/n': 50, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0005731582641601562}\n",
      "[flaml.tune.tune: 03-22 14:20:21] {811} INFO - trial 23 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9112105335374863}, 'max_tokens': 202, 'n': 23, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:20:40] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.5615979999999999, 'cost': 0.018048, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9112105335374863}, 'max_tokens': 202, 'n': 23, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9112105335374863}, 'config/max_tokens': 202, 'config/n': 23, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 19.577951192855835}\n",
      "[flaml.tune.tune: 03-22 14:20:40] {811} INFO - trial 24 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3910669258125473}, 'max_tokens': 182, 'n': 12, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:22:30] {215} INFO - result: {'expected_success': 0.8907601699971748, 'success': 0.9, 'success_vote': 0.85, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.6253439999999999, 'cost': 0.063746, 'inference_cost': 0.0031873000000000005, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.3910669258125473}, 'max_tokens': 182, 'n': 12, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.3910669258125473}, 'config/max_tokens': 182, 'config/n': 12, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 109.24226975440979}\n",
      "[flaml.tune.tune: 03-22 14:22:30] {811} INFO - trial 25 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.4011546010577773}, 'max_tokens': 171, 'n': 18, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:23:04] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.6597660000000001, 'cost': 0.034422, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.4011546010577773}, 'max_tokens': 171, 'n': 18, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.4011546010577773}, 'config/max_tokens': 171, 'config/n': 18, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 34.612515687942505}\n",
      "[flaml.tune.tune: 03-22 14:23:04] {811} INFO - trial 26 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.23162454328453796}, 'max_tokens': 267, 'n': 1, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:25:00] {215} INFO - result: {'expected_success': 0.55, 'success': 0.55, 'success_vote': 0.55, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.6691520000000001, 'cost': 0.009386, 'inference_cost': 0.00046929999999999997, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.23162454328453796}, 'max_tokens': 267, 'n': 1, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.23162454328453796}, 'config/max_tokens': 267, 'config/n': 1, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 115.30330491065979}\n",
      "[flaml.tune.tune: 03-22 14:25:00] {811} INFO - trial 27 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.18263855377383265}, 'max_tokens': 108, 'n': 11, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:26:11] {215} INFO - result: {'expected_success': 0.5239008355441492, 'success': 0.55, 'success_vote': 0.55, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.714798, 'cost': 0.045646, 'inference_cost': 0.0022823, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.18263855377383265}, 'max_tokens': 108, 'n': 11, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.18263855377383265}, 'config/max_tokens': 108, 'config/n': 11, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 71.19590616226196}\n",
      "[flaml.tune.tune: 03-22 14:26:11] {811} INFO - trial 28 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.4729697871536633}, 'max_tokens': 71, 'n': 28, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:27:20] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.7951680000000002, 'cost': 0.08037, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.4729697871536633}, 'max_tokens': 71, 'n': 28, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.4729697871536633}, 'config/max_tokens': 71, 'config/n': 28, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 69.35182213783264}\n",
      "[flaml.tune.tune: 03-22 14:27:20] {811} INFO - trial 29 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9990495004030453}, 'max_tokens': 165, 'n': 16, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:28:40] {215} INFO - result: {'success_vote': 0, 'total_cost': 0.8617020000000002, 'cost': 0.06653400000000001, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'top_p': 0.9990495004030453}, 'max_tokens': 165, 'n': 16, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'top_p': 0.9990495004030453}, 'config/max_tokens': 165, 'config/n': 16, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 79.99001264572144}\n",
      "[flaml.tune.tune: 03-22 14:28:40] {811} INFO - trial 30 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.23855336406992245}, 'max_tokens': 473, 'n': 7, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:30:30] {215} INFO - result: {'expected_success': 0.8733691501233087, 'success': 0.9, 'success_vote': 0.75, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.9065980000000002, 'cost': 0.04489599999999999, 'inference_cost': 0.0022448, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.23855336406992245}, 'max_tokens': 473, 'n': 7, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.23855336406992245}, 'config/max_tokens': 473, 'config/n': 7, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 110.37087178230286}\n",
      "[flaml.tune.tune: 03-22 14:30:30] {811} INFO - trial 31 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.48661641609109463}, 'max_tokens': 174, 'n': 49, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:30:30] {215} INFO - result: {'inference_cost': inf, 'success_vote': -inf, 'cost': 0, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.48661641609109463}, 'max_tokens': 174, 'n': 49, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.48661641609109463}, 'config/max_tokens': 174, 'config/n': 49, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 0.0006039142608642578}\n",
      "[flaml.tune.tune: 03-22 14:30:30] {811} INFO - trial 32 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6877466127078208}, 'max_tokens': 277, 'n': 9, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:32:39] {215} INFO - result: {'expected_success': 0.8447189393486104, 'success': 0.85, 'success_vote': 0.8, 'voted_answer': '\\n\\nWe have that\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = |-3| \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.9648620000000004, 'cost': 0.05826400000000001, 'inference_cost': 0.0029132, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6877466127078208}, 'max_tokens': 277, 'n': 9, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6877466127078208}, 'config/max_tokens': 277, 'config/n': 9, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 128.33817434310913}\n",
      "[flaml.tune.tune: 03-22 14:32:39] {811} INFO - trial 33 config: {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6987697694938808}, 'max_tokens': 286, 'n': 8, 'prompt': 0}\n",
      "[flaml.tune.tune: 03-22 14:34:10] {215} INFO - result: {'success_vote': 0, 'total_cost': 1.0001280000000006, 'cost': 0.035266, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.6987697694938808}, 'max_tokens': 286, 'n': 8, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.6987697694938808}, 'config/max_tokens': 286, 'config/n': 8, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 91.10532450675964}\n",
      "[flaml.tune.tune: 03-22 14:34:10] {834} WARNING - fail to sample a trial for 100 times in a row, stopping.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "config, analysis = oai.ChatCompletion.tune(\n",
    "    data=tune_data,  # the data for tuning\n",
    "    metric=\"success_vote\",  # the metric to optimize\n",
    "    mode=\"max\",  # the optimization mode\n",
    "    eval_func=success_metrics,  # the evaluation function to return the success metrics\n",
    "    # log_file_name=\"logs/math.log\",  # the log file name\n",
    "    inference_budget=0.004,  # the inference budget (dollar)\n",
    "    optimization_budget=1,  # the optimization budget (dollar)\n",
    "    # num_samples can further limit the number of trials for different hyperparameter configurations;\n",
    "    # -1 means decided by the optimization budget only\n",
    "    num_samples=-1,\n",
    "    # model=\"chatgpt-35-turbo-0301\",  # uncomment if using Azure OpenAI\n",
    "    prompt=prompts,  # the prompt templates to choose from\n",
    "    # stop=\"###\",  # the stop sequence\n",
    "    logging_level=logging.INFO,  # the logging level\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output tuning results\n",
    "\n",
    "After the tuning, we can print out the config and the result found by FLAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:55.049204Z",
     "iopub.status.busy": "2023-02-13T23:41:55.048871Z",
     "iopub.status.idle": "2023-02-13T23:41:55.053284Z",
     "shell.execute_reply": "2023-02-13T23:41:55.052574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized config {'model': 'gpt-3.5-turbo', 'max_tokens': 129, 'n': 9, 'prompt': '{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.', 'stop': None, 'temperature': 0.9086488808086682}\n",
      "best result on tuning data {'expected_success': 0.808418317029175, 'success': 0.85, 'success_vote': 0.85, 'voted_answer': '\\n\\nWe have that $\\\\|\\\\mathbf{v}\\\\| = 4,$ so\\n\\\\[\\\\|-3 \\\\mathbf{v}\\\\| = 3 \\\\|\\\\mathbf{v}\\\\| = 3 \\\\cdot 4 = \\\\boxed{12}.\\\\]', 'total_cost': 0.16798400000000008, 'cost': 0.041768, 'inference_cost': 0.0020884000000000002, 'training_iteration': 0, 'config': {'model': 'gpt-3.5-turbo', 'temperature_or_top_p': {'temperature': 0.9086488808086682}, 'max_tokens': 129, 'n': 9, 'prompt': 0}, 'config/model': 'gpt-3.5-turbo', 'config/temperature_or_top_p': {'temperature': 0.9086488808086682}, 'config/max_tokens': 129, 'config/n': 9, 'config/prompt': 0, 'experiment_tag': 'exp', 'time_total_s': 75.71505641937256}\n"
     ]
    }
   ],
   "source": [
    "print(\"optimized config\", config)\n",
    "print(\"best result on tuning data\", analysis.best_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Make a request with the tuned config\n",
    "\n",
    "We can apply the tuned config on the request for an example task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:55.056205Z",
     "iopub.status.busy": "2023-02-13T23:41:55.055631Z",
     "iopub.status.idle": "2023-02-13T23:41:56.039259Z",
     "shell.execute_reply": "2023-02-13T23:41:56.038427Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the logarithmic identity $\\\\log_{a}{b} + \\\\log_{a}{c} = \\\\log_{a}{bc}$, so $$\\\\log_{10}{40} + \\\\log_{10}{25} = \\\\log_{10}{(40 \\\\cdot 25)}.$$Calculation shows that $40\\\\cdot 25 = 1000$, so $\\\\log_{10}{(40 \\\\cdot 25)} = \\\\boxed{3}$.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe have $\\\\log_{10} 40 = \\\\log_{10} (2^3 \\\\cdot 5) = \\\\log_{10} 2^3 + \\\\log_{10} 5 = 3\\\\log_{10} 2 + \\\\log_{10} 5$.\\n\\nAlso, $\\\\log_{10} 25 = \\\\log_{10} 5^2 = 2\\\\log_{10} 5$.\\n\\nTherefore, $\\\\log_{10} 40 +\\\\log_{10} 25 = 3\\\\log_{10} 2 + \\\\log_{10} 5\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 2,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the identity $\\\\log_{a} x + \\\\log_{a} y = \\\\log_{a} (xy)$, which states that the sum of the logarithms base $a$ of two numbers $x$ and $y$ is equal to the logarithm base $a$ of their product $xy$. Applying this to $\\\\log_{10} 40 + \\\\log_{10} 25$, we get $$\\\\log_{10} (40\\\\cdot25)=\\\\log_{10} 1000=3.$$Therefore, $\\\\log_{10} 40 +\\\\log_{10} 25=\\\\\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 3,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the logarithmic identity $\\\\log_{a} b+\\\\log_{a} c=\\\\log_{a} (bc)$.\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40\\\\cdot 25)\\\\\\\\\\n&= \\\\log_{10} 1000\\\\\\\\\\n&=\\\\boxed{3}.\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 4,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\n\\n\\nWe use the properties $\\\\log_{a}b+\\\\log_{a}c=\\\\log_{a}(bc)$ and $\\\\log_{a^n}b^n=\\\\log_{a}b$ to write \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &=\\\\log_{10} (4\\\\cdot10)+\\\\log_{10} (5\\\\cdot5) \\\\\\\\\\n&= \\\\log_{10}(2^2\\\\cdot5\\\\cdot2\\\\cdot5)\\\\\\\\\\n&=\\\\log_{10}(2^3\\\\cdot5^2)\\\\\\\\\\n&=\\\\log\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 5,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nUsing the product rule of logarithms, we have:\\n\\n\\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40 \\\\cdot 25) \\\\\\\\\\n&= \\\\log_{10} (1000) \\\\\\\\\\n&= \\\\boxed{3}\\n\\\\end{align*}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 6,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\n\\n\\n\\nUsing the product-to-sum formula, we can simplify $\\\\log_{10} 40 +\\\\log_{10} 25$ as follows: $$\\\\log_{10} 40 +\\\\log_{10} 25 = \\\\log_{10} (40 \\\\cdot 25) = \\\\log_{10} 1000.$$ Since $1000 = 10^3$, we have $\\\\log_{10} 1000 = \\\\boxed{3}$.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 7,\n",
      "      \"message\": {\n",
      "        \"content\": \"Using the product property of logarithms, we can combine the two logarithms: \\\\begin{align*}\\n\\\\log_{10} 40 +\\\\log_{10} 25 &= \\\\log_{10} (40\\\\cdot 25)\\\\\\\\\\n&= \\\\log_{10} (1000)\\\\\\\\\\n&=\\\\boxed{3}.\\n\\\\end{align*}Note that $40\\\\cdot25=1000$ is easily obtained by prime factorizing: $40=2^3\\\\cdot5$ and $25=5^2$, so $40\\\\cdot25 = 2^3\\\\cdot5\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 8,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\n\\nWe use the rule $\\\\log_{a} b + \\\\log_{a} c = \\\\log_{a} bc$ to combine the given logarithms: $$\\\\log_{10} 40 + \\\\log_{10} 25 = \\\\log_{10} (40\\\\cdot 25).$$ We then simplify the product inside the logarithm: $$40\\\\cdot 25 = 4\\\\cdot 10\\\\cdot 5\\\\cdot 5 = 2^2 \\\\cdot 2\\\\cdot 5^2.$$ Finally, we use the rule $\\\\log_{a} b^c = c\\\\cdot \\\\\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1679494128,\n",
      "  \"id\": \"chatcmpl-6wtQuBOikY7oai9YHrFTpiuhtNDAT\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 998,\n",
      "    \"prompt_tokens\": 46,\n",
      "    \"total_tokens\": 1044\n",
      "  }\n",
      "}\n",
      "{'expected_success': 0.9993233605154012, 'success': True, 'success_vote': 1.0, 'voted_answer': '\\n\\nWe use the logarithmic identity $\\\\log_{a}{b} + \\\\log_{a}{c} = \\\\log_{a}{bc}$, so $$\\\\log_{10}{40} + \\\\log_{10}{25} = \\\\log_{10}{(40 \\\\cdot 25)}.$$Calculation shows that $40\\\\cdot 25 = 1000$, so $\\\\log_{10}{(40 \\\\cdot 25)} = \\\\boxed{3}$.'}\n"
     ]
    }
   ],
   "source": [
    "responses = oai.ChatCompletion.create(context=tune_data[1], **config)\n",
    "print(responses)\n",
    "print(success_metrics([response[\"message\"][\"content\"].rstrip() for response in responses[\"choices\"]], **tune_data[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the success rate on the test data\n",
    "\n",
    "You can use flaml's `oai.ChatCompletion.eval` to evaluate the performance of an entire dataset with the tuned config. To do that you need to set `oai.ChatCompletion.data` to the data to evaluate. The following code will take a while (~30mins) to evaluate all the 438 test data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:56.042764Z",
     "iopub.status.busy": "2023-02-13T23:41:56.042086Z",
     "iopub.status.idle": "2023-02-13T23:53:05.597643Z",
     "shell.execute_reply": "2023-02-13T23:53:05.596603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expected_success': 0.7718961191261685, 'success': 0.816933638443936, 'success_vote': 0.7780320366132724, 'voted_answer': '\\n\\nLet $x = \\\\arccos \\\\frac{1}{3}.$  Then $\\\\cos x = \\\\frac{1}{3},$ so by Pythagoras,\\n\\\\[\\\\sin^2 x = 1 - \\\\frac{1}{9} = \\\\frac{8}{9}.\\\\]Since $\\\\sin x$ is positive and $\\\\cos x$ is positive, $\\\\tan x$ is positive.  Therefore,\\n\\\\[\\\\tan x = \\\\frac{\\\\sin x}{\\\\cos x} = \\\\frac{\\\\sqrt{8/9}}{1/3} = \\\\boxed{3 \\\\sqrt{2}}', 'total_cost': 1.8701819999999996, 'cost': 0.8700540000000007, 'inference_cost': 0.001990970251716247}\n"
     ]
    }
   ],
   "source": [
    "oai.ChatCompletion.data = test_data\n",
    "result = oai.ChatCompletion.eval(analysis.best_config, prune=False, eval_only=True)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the default, untuned config (with the same prompt as the tuned config)? We can evaluate it and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expected_success': 0.6887871853546911, 'success': 0.6887871853546911, 'success_vote': 0.6887871853546911, 'voted_answer': '\\n\\n\\n\\n\\n\\n\\n\\n\\nLet $y = \\\\arccos \\\\frac{1}{3},$ so $0 \\\\le y \\\\le \\\\pi$ and $\\\\cos y = \\\\frac{1}{3}.$  Then $\\\\sin^2 y = 1 - \\\\cos^2 y = \\\\frac{8}{9},$ so $\\\\sin y = \\\\sqrt{\\\\frac{8}{9}} = \\\\frac{2 \\\\sqrt{2}}{3},$ and\\n\\\\[\\\\tan y = \\\\frac{\\\\sin y}{\\\\cos y} = \\\\frac{\\\\frac{2 \\\\sqrt{2}}{3}}{\\\\frac{1}{3}} = \\\\boxed{2 \\\\sqrt{2}}.\\\\]', 'total_cost': 2.0723560000000005, 'cost': 0.20217400000000005, 'inference_cost': 0.0004626407322654462}\n"
     ]
    }
   ],
   "source": [
    "default_config = {\"model\": 'gpt-3.5-turbo', \"prompt\": 0}\n",
    "default_result = oai.ChatCompletion.eval(default_config, prune=False, eval_only=True)\n",
    "print(default_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned config succeeds in 77.8% test cases\n",
      "untuned config succeeds in 68.9% test cases\n"
     ]
    }
   ],
   "source": [
    "print(\"tuned config succeeds in {:.1f}% test cases\".format(result[\"success_vote\"] * 100))\n",
    "print(\"untuned config succeeds in {:.1f}% test cases\".format(default_result[\"success_vote\"] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2d910cfd2d2a4fc49fc30fbbdc5576a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "454146d0f7224f038689031002906e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4ae2b6f5a974fd4bafb6abb9d12ff26",
        "IPY_MODEL_577e1e3cc4db4942b0883577b3b52755",
        "IPY_MODEL_b40bdfb1ac1d4cffb7cefcb870c64d45"
       ],
       "layout": "IPY_MODEL_dc83c7bff2f241309537a8119dfc7555",
       "tabbable": null,
       "tooltip": null
      }
     },
     "577e1e3cc4db4942b0883577b3b52755": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d910cfd2d2a4fc49fc30fbbdc5576a7",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74a6ba0c3cbc4051be0a83e152fe1e62",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "6086462a12d54bafa59d3c4566f06cb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74a6ba0c3cbc4051be0a83e152fe1e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d3f3d9e15894d05a4d188ff4f466554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b40bdfb1ac1d4cffb7cefcb870c64d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1355871cc6f4dd4b50d9df5af20e5c8",
       "placeholder": "​",
       "style": "IPY_MODEL_ca245376fd9f4354af6b2befe4af4466",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 44.69it/s]"
      }
     },
     "ca245376fd9f4354af6b2befe4af4466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc83c7bff2f241309537a8119dfc7555": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4ae2b6f5a974fd4bafb6abb9d12ff26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6086462a12d54bafa59d3c4566f06cb2",
       "placeholder": "​",
       "style": "IPY_MODEL_7d3f3d9e15894d05a4d188ff4f466554",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "f1355871cc6f4dd4b50d9df5af20e5c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
